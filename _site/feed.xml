<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-23T09:33:06+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JJ’s Blog</title><subtitle>Automation, Containers, Cloud Infrastructure, Cloud Native Tools, DevSecOps, Microservices Architecture and Scalable Solutions.</subtitle><author><name>Jia Jun Ng (JJ)</name></author><entry><title type="html">Integrate Snyk Into Google Cloud Build</title><link href="http://localhost:4000/2021/12/22/integrate-snyk-into-google-cloud-build" rel="alternate" type="text/html" title="Integrate Snyk Into Google Cloud Build" /><published>2021-12-22T00:00:00+08:00</published><updated>2021-12-22T00:00:00+08:00</updated><id>http://localhost:4000/2021/12/22/Integrate-Snyk-Into-Google-Cloud-Build</id><content type="html" xml:base="http://localhost:4000/2021/12/22/integrate-snyk-into-google-cloud-build">&lt;p&gt;Are you using Google Cloud Build as your CI/CD pipeline to build your applications? If so, how do you mandate security across the different stages?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147042759-5e92873d-e1db-41f8-ad09-65f410698766.png&quot; alt=&quot;Snyk Google Cloud Build&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this post today, I will be sharing on how to integrate Snyk into Google Cloud Build as there isn’t any native support available yet.&lt;/p&gt;

&lt;p&gt;There are way too many options when it comes to choosing your pipeline engine. The obvious choice should be of a balance of integration options and team expertise.
Previously, I have written blog posts on &lt;a href=&quot;https://jjblog.io/2020/10/14/intro-to-devops&quot;&gt;DevOps&lt;/a&gt; and
&lt;a href=&quot;https://jjblog.io/2021/03/04/intro-to-devsecops&quot;&gt;DevSecOps&lt;/a&gt;, and have shared examples for some of the tools to set up the pipeline. So why Google Cloud Build?
I believe it is because of its native cloud nature (serverless) which makes it simple to set up. I do have seen cloud native organisations moving towards Google Cloud Platform (GCP) with many having to like the idea of using
with the GCP stack which includes the Google Cloud Build.&lt;/p&gt;

&lt;p&gt;Today’s topic is on Snyk integration; although Snyk supports a wide variety of CI/CD tools (e.g Jenkins) but there isn’t native support for Google Cloud Build yet.
In such cases, the integration with Snyk will have to be through the Snyk command-line interface (Snyk CLI).&lt;/p&gt;

&lt;h2 id=&quot;what-is-snyk-cli&quot;&gt;What is Snyk CLI&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147044601-7770a8e8-e353-42ec-867f-59d4d90dda57.png&quot; alt=&quot;Snyk CLI&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://docs.snyk.io/features/snyk-cli&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;strong&gt;Snyk CLI&lt;/strong&gt; brings functionality of Snyk into your development workflow. It can be run locally, or in your CI/CD pipeline, to scan your projects for security issues.” - Snyk&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Snyk CLI allows “plug and play” integration to any pipeline workflow with the flexibility to trigger a scan in different stages.
Snyk CLI is a Node.JS application which you can install using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yarn&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are other methods to install such as using a Docker image which I will be using in my setup.
In the official documentation &lt;a href=&quot;https://docs.snyk.io/features/snyk-cli/install-the-snyk-cli&quot;&gt;here&lt;/a&gt;, you will be able to find the different installation methods.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In my setup, I will be building a Node.js application which is hosted on my personal GitHub, &lt;a href=&quot;https://github.com/jiajunngjj/goof&quot;&gt;https://github.com/jiajunngjj/goof&lt;/a&gt;.
I will be running Snyk Container in Google Cloud Build to run a couple of Snyk tests :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Code analysis (SAST)&lt;/li&gt;
  &lt;li&gt;Software composition analysis (SCA)&lt;/li&gt;
  &lt;li&gt;Scanning of container images&lt;/li&gt;
  &lt;li&gt;Scanning of infrastructure as code (IaC) files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, I will show how to generate a HTML report for my SCA portion using &lt;a href=&quot;https://github.com/snyk/snyk-to-html&quot;&gt;snyk-to-html&lt;/a&gt;, a Snyk utility to convert JSON output to HTML,
and store the artifact in my Google Cloud Storage.&lt;/p&gt;

&lt;h3 id=&quot;snyk-setup&quot;&gt;Snyk Setup&lt;/h3&gt;
&lt;p&gt;On Snyk Platform, retrieve the access token from the service account (sign up for an account if you don’t have one).
Go to Settings -&amp;gt; Service accounts, create a new service account to generate the token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147032928-c484b746-2bae-42b6-a3c6-338da9640543.png&quot; alt=&quot;Snyk Settings&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;google-cloud-build-setup&quot;&gt;Google Cloud Build Setup&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In the GCP console, navigate to Cloud Build. Go to Triggers, creates a new trigger.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the Trigger page, add the Node.js application GitHub repository under Source. Follow the steps in Connect repository to connect.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the Trigger page, add a variable, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_SNYK_TOKEN&lt;/code&gt;, with the access token from Snyk Setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the Trigger page, create the Inline YAML with the following content under Configuration (&lt;em&gt;cloudbuild.yaml&lt;/em&gt;):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;run-snyk-open-source-test&quot;&gt;Run Snyk Open Source Test&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk test --severity-threshold=medium
    id: Snyk Open Source Test
    entrypoint: bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;run-snyk-code-test&quot;&gt;Run Snyk Code Test&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk code test --severity-threshold=medium
    id: Snyk Code Test
    entrypoint: bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;run-snyk-container-test&quot;&gt;Run Snyk Container Test&lt;/h4&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk container test --severity-threshold=medium jiajunngjj/docker-goof:latest
    id: Snyk Container Test
    entrypoint: bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;run-snyk-iac-test&quot;&gt;Run Snyk IaC Test&lt;/h4&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk iac test --severity-threshold=medium main.tf
    id: Snyk IaC Test
    entrypoint: bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;create-html-report-and-store-it-in-your-google-cloud-storage&quot;&gt;Create HTML report and store it in your Google Cloud Storage&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        set -o pipefail
        snyk test --severity-threshold=medium --json | snyk-to-html -o results.html || true
    id: Create HTML artifact
    entrypoint: bash
artifacts:
  objects:
    location: 'gs://jjng-store/scan_output'
    paths:
      - results.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here are some notes for my configurations above:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Snyk Docker image, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snyk/snyk-cli:npm&lt;/code&gt;, is used to run Snyk CLI&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--severity-threshold=medium&lt;/code&gt; to flag out vulnerabilities of medium or higher severity, meaning if there is any of vulnerabilities of severity medium or higher, the build will fail&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|| true&lt;/code&gt; to provide exit code 0 as the build needs to be successful in order to save the artifact in the Google Cloud Storage&lt;/li&gt;
  &lt;li&gt;The assumption that you have built your container image and pushed it to a container registry (Docker Hub in my example) hence you’re running
the image scan by specifying the container image directly on the registry&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the combined version of the above Snyk tests, which makes sense in the software development lifecycle (SDLC) where you scan application codebase for SAST and SCA, scan container base images before building your containers, and scan the IaC files before provisioning the infrastructure:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;steps:
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk test --severity-threshold=medium || true
    id: Snyk Open Source Test
    entrypoint: bash
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk code test --severity-threshold=medium || true
    id: Snyk Code Test
    entrypoint: bash
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk container test --severity-threshold=medium jiajunngjj/docker-goof:latest
    id: Snyk Container Test
    entrypoint: bash
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        snyk iac test --severity-threshold=medium main.tf || true
    id: Snyk IaC Test
    entrypoint: bash
  - name: snyk/snyk-cli:npm
    args:
      - '-c'
      - |-
        snyk config set api=${_SNYK_TOKEN}
        set -o pipefail
        snyk test --severity-threshold=medium --json | snyk-to-html -o results.html || true
    id: Create HTML artifact
    entrypoint: bash
artifacts:
  objects:
    location: 'gs://jjng-store/scan_output'
    paths:
      - results.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that I have set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|| true&lt;/code&gt; above to make sure all tests pass.&lt;/p&gt;

&lt;h2 id=&quot;sample-output&quot;&gt;Sample Output&lt;/h2&gt;
&lt;p&gt;Here is a sample output of a failed build of Snyk Code Test:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147050780-e81c535b-0178-4a30-ab40-8c43acc7874f.png&quot; alt=&quot;Failed build&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a sample output of my Google Cloud Storage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147051058-23a77950-92bd-40b2-8105-eb5e03d98adf.png&quot; alt=&quot;HTML report storage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a sample output of my HTML artifact:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/147051128-4044c783-9383-4949-89c0-1cb997ab847e.png&quot; alt=&quot;HTML report&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Mandating security is a must when it comes to building software, especially in the agile world today where there are a lot of moving pieces. Snyk CLI helps to streamline various security tests in the SDLC, covering different
aspects of security from SAST and SCA to building container and provisioning infrastructure.&lt;/p&gt;

&lt;p&gt;Most importantly, Snyk CLI makes it possible to integrate with any CI/CD tool and all these tests can be automated (just like the example above), bringing agility to the development team.&lt;/p&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="DevSecOps" /><category term="Devops" /><category term="GCP" /><category term="Snyk" /><category term="GoogleCloudBuild" /><summary type="html">Are you using Google Cloud Build as your CI/CD pipeline to build your applications? If so, how do you mandate security across the different stages?</summary></entry><entry><title type="html">Why vSphere with Tanzu?</title><link href="http://localhost:4000/2021/06/15/why-vsphere-with-tanzu" rel="alternate" type="text/html" title="Why vSphere with Tanzu?" /><published>2021-06-15T00:00:00+08:00</published><updated>2021-06-15T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/15/Why-vSphere-with-Tanzu</id><content type="html" xml:base="http://localhost:4000/2021/06/15/why-vsphere-with-tanzu">&lt;p&gt;Are you running your own Kubernetes cluster already? If so, how are you managing it currently?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/121776669-c25d8400-cbc0-11eb-9e03-1740919b10f3.png&quot; alt=&quot;kubernetes cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes was born in 2014 and since then, it has become the default way of managing and scaling containerized applications. Many organisations have begun their journey
towards microservices architecture and scalable solutions for their applications, going the cloud native way. Kubernetes plays a part in the cloud native journey alongside with many other open source tools, not to mention the
native cloud providers which provide the underlying infrastructure.&lt;/p&gt;

&lt;p&gt;During this journey, many infrastructure and operation folks face steep learning curves in managing and operating Kubernetes while the developers are happily exploring
the new way of application development, maybe. Kubernetes is a complicated and interesting platform on their own and every add-on can be an open source project. Essential functionalities like monitoring and 
logging in Kubernetes are also achieved through open source projects. Of course if you have enterprise software which provides logging feature, you can integrate with Kubernetes as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/122045558-7c96fa80-ce10-11eb-9e9f-ed84f217721a.png&quot; alt=&quot;CNCF Landscape&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://landscape.cncf.io/&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Above is a picture of the cloud native landscape from Cloud Native Computing Foundation (CNCF). This landscape of open source projects 
provides end-user communities with viable options for building cloud native applications. This should give you an idea of the complexity
one has to go through to go the cloud native way.&lt;/p&gt;

&lt;p&gt;Embarking on a journey with Kubernetes is just a start of cloud native development, and many software vendors have already released 
its own distribution of Kubernetes, packaging with its own selected list of open source projects, providing supportability.&lt;/p&gt;

&lt;p&gt;There are many vendors to choose from, and today, I would like to share a unique feature from the VMware Tanzu portfolio, vSphere with Tanzu, which addresses the steep learning curve of Kubernetes from an
infrastructure perspective.&lt;/p&gt;

&lt;h2 id=&quot;what-is-vsphere-with-tanzu&quot;&gt;What is vSphere with Tanzu&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/121643488-74635600-cac4-11eb-9d0b-3d502ce9a67a.png&quot; alt=&quot;vsphere and kubernetes&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;strong&gt;vSphere with Tanzu&lt;/strong&gt; is the new generation of vSphere for containerized applications. This single, streamlined solution bridges the gap between IT operations and developers with a new kind of infrastructure for modern, cloud-native applications both on premises and in public clouds.” - VMware&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;vSphere with Tanzu integrates Kubernetes and containers into the vSphere ecosystem, providing an unique capability to run Kubernetes workloads directly on ESXi hosts.&lt;/p&gt;

&lt;p&gt;With the Tanzu feature turned on in vSphere, you can start running Kubernetes in two ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;run containers directly in ESXi host via &lt;strong&gt;vSphere Pod&lt;/strong&gt; service (requires NSX-T)&lt;/li&gt;
  &lt;li&gt;run Tanzu Kubernetes cluster via &lt;strong&gt;Tanzu Kubernetes Grid Service&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vsphere-pod&quot;&gt;vSphere Pod&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;“&lt;strong&gt;vSphere Pod&lt;/strong&gt; allows container workloads to be deployed directly in the ESXi host. It is Open Container Initiative (OCI) compatible and can run containers from any operating system as long as the containers are OCI compatible.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For a detailed explanation on vSphere Pod, you can visit &lt;a href=&quot;https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-276F809D-2015-4FC6-92D8-8539D491815E.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;tanzu-kubernetes-grid-service&quot;&gt;Tanzu Kubernetes Grid Service&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;“&lt;strong&gt;Tanzu Kubernetes Grid Service&lt;/strong&gt; allows Kubernetes cluster to be deployed or consumed in a self-service manner.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For a detailed explanation on Tanzu Kubernetes Grid service, you can visit &lt;a href=&quot;https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-4D0D375F-C001-4F1D-AAB1-1789C5577A94.html#GUID-4D0D375F-C001-4F1D-AAB1-1789C5577A94&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/121887969-0334d480-cd4a-11eb-9f99-18816e7be2eb.png&quot; alt=&quot;Vsphere with Tanzu Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the Tanzu feature turned on in vSphere:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kubernetes control planes are deployed into the ESXi hosts which enables the capability to run Kubernetes workloads within ESXi&lt;/li&gt;
  &lt;li&gt;It creates a Supervisor cluster, a layer which overlays the ESXi hosts&lt;/li&gt;
  &lt;li&gt;Within the Supervisor cluster, an administrator would need to create vSphere namespace to either run vSphere Pods or Tanzu Kubernetes cluster, which provides quota management and isolation, very similar to &lt;a href=&quot;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&quot;&gt;Kubernetes’s namespace concept&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-case&quot;&gt;Use case&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/122066698-8bd47300-ce25-11eb-805c-2710a9ab3ee0.png&quot; alt=&quot;usecase1-vsphere and kubernetes workload&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;Use case #1&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;For organizations who are running a lot of virtual machine (VM) workloads and are looking to start modernizing their applications, vSphere with Tanzu removes the need to go through complicated steps to create a Kubernetes cluster manually. It also provides a single
pane of glass to manage both VM and container workloads. This is an awesome experience from an operating perspective where you can manage resources for Kubernetes workloads the same way VM resources are managed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/122068503-0b167680-ce27-11eb-8f14-df59bb3bdffb.png&quot; alt=&quot;usecase2&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;Use case #2&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Apart from organizations who are starting their Kubernetes journey, vSphere with Tanzu is also ideal as an environment setup for testing/development even if one already has a production Kubernetes cluster. One key feature on vSphere is that the Tanzu Kubernetes cluster can be provisioned
in a self-service manner which is highly efficient for testing applications. This could possibly reduce the need for an infrastructure as code (IaC) tool to a certain extent as vSphere takes care of it now. Also, vSphere with Tanzu comes with many out of the box capabilities like load balancing, container networking, container registry etc. All these features make the setup of a 
Kubernetes cluster easy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/122068908-5b8dd400-ce27-11eb-8644-59d11576468f.png&quot; alt=&quot;usecase3-tanzu advanced&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://tanzu.vmware.com/tanzu/advanced&quot;&gt;Use case #3&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;For organizations who are looking to run a production Kubernetes cluster, vSphere with Tanzu can also be a starting point because you can extend the capabilities whenever you need it. Referring back to the CNCF landscape, with so many tools out there, the question is how do you know what you need? 
You will only know when you start using it. Tanzu Kubernetes comes with different editions where you can add on the capabilities. The concept here is to start small, and grow it when the capability is there.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The idea is to have a single platform to manage different kinds of applications, from monolithic applications to cloud native
applications. With Kubernetes integrated into the vSphere ecosystem, Kubernetes workload can leverage on vSphere features (vMotion, Distributed Resource Scheduler (DRS), etc) to further provide stability to the applications.&lt;/p&gt;

&lt;p&gt;vSphere with Tanzu is a good start for organizations who are looking to kickstart their Kubernetes journey. 
Depending on the Kubernetes capability, this feature can be a bridge to the containers world, upskilling the infrastructure folks, bringing the organization forward together as one unit.&lt;/p&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="kubernetes" /><category term="containers" /><category term="vsphere" /><category term="vmware" /><category term="tanzu" /><summary type="html">Are you running your own Kubernetes cluster already? If so, how are you managing it currently?</summary></entry><entry><title type="html">Adding Salt To Your Environment</title><link href="http://localhost:4000/2021/04/16/add-salt-to-your-environment" rel="alternate" type="text/html" title="Adding Salt To Your Environment" /><published>2021-04-16T00:00:00+08:00</published><updated>2021-04-16T00:00:00+08:00</updated><id>http://localhost:4000/2021/04/16/Adding-Salt-To-Your-Environment</id><content type="html" xml:base="http://localhost:4000/2021/04/16/add-salt-to-your-environment">&lt;p&gt;Have you added “Salt” to your environment? Not the condiment, salt. I meant SaltStack’s Salt…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/114913607-f7b15400-9e53-11eb-91a9-7a3694de1b1f.png&quot; alt=&quot;Salt Shaker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IT automation and configuration management tools serve as an important foundation of an IT day-to-day operation. Especially in today’s world, where organisations are moving towards agile practices to deliver
applications faster. These responsibilities are often given to the development teams, empowering them to provision, configure, and manage their own environment. Hence, it is important to have the right tool which fulfills
 the needs of both infra/ops and development teams. Basically it is about giving infra/ops team enough management power to manage what development team can manage.&lt;/p&gt;

&lt;p&gt;There are many tools out there which does the job and in this post, I will be sharing about SaltStack, an open source tool which was acquired by VMware in 2020, and went GA recently.&lt;/p&gt;

&lt;h2 id=&quot;what-is-saltstack&quot;&gt;What is SaltStack&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/114822677-01f03580-9df5-11eb-8224-b8de8d5dc8b1.png&quot; alt=&quot;SaltStack&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;strong&gt;SaltStack, also known as Salt&lt;/strong&gt; is a Python-based, open-source software for event-driven IT automation, remote task execution, and configuration management. “ - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Salt automates the configuration of your infrastructure from on-premise to cloud. Repetitive manual system administrative tasks or processes can be automated with Salt to reduce unnecessary errors when it comes to configuring 
systems. It simplifies management of systems as you can execute commands remotely to multiple machines directly in parallel. Not to mention that Salt supports different operating systems, including various distributions of GNU/Linux, Microsoft Windows, and macOS.&lt;/p&gt;

&lt;p&gt;Salt also leverages on YAML programming language, and most of the time, users will be writing in YAML for their configurations unless users would like to write their own modules in Python.&lt;/p&gt;

&lt;h2 id=&quot;saltstack-architecture&quot;&gt;SaltStack Architecture&lt;/h2&gt;
&lt;p&gt;Salt uses a publisher-subscriber model and follows a concept of “&lt;strong&gt;Master&lt;/strong&gt;” and “&lt;strong&gt;Minion&lt;/strong&gt;”, where Master is the controlling node and Minion is the node that is being controlled. Master and Minion daemon service will be installed upon setting up Salt.&lt;/p&gt;

&lt;p&gt;Communication between Master and Minion is through an event bus backed by &lt;a href=&quot;https://zeromq.org/&quot;&gt;ZeroMQ&lt;/a&gt; which does not require any message broker and creates an asynchronous network topology to provide the fastest communication possible.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;1016&quot; alt=&quot;Salt architecture&quot; src=&quot;https://user-images.githubusercontent.com/25560159/114836757-8ea2ef80-9e05-11eb-90f6-759791f90a31.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding on to the above architecture, multi master can be configured to provide high availability.&lt;/p&gt;

&lt;p&gt;For users who do not like the idea of having an agent, agentless Salt is available as well. In such setup, minion daemon does not need to be installed in the remote host. Instead, the only requirements on the remote host are SSH and Python.
Agentless Salt can be set up in conjunction with a master-minion environment as well.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;756&quot; alt=&quot;Salt agentless&quot; src=&quot;https://user-images.githubusercontent.com/25560159/114838796-a7140980-9e07-11eb-91ed-f94d68ad2db2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The recommended setup is to operate in a Master and Minion approach as execution via SSH is substantially slower, existing users would have already experienced this when managing large number of nodes.&lt;/p&gt;

&lt;h2 id=&quot;saltstack-use-case&quot;&gt;SaltStack Use Case&lt;/h2&gt;
&lt;p&gt;As Salt is extremely modular and flexible, it can work pretty much with any use case relating to DevOps, SecOps, NetOps, or CloudOps.&lt;/p&gt;

&lt;p&gt;Let me share some use cases:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Apart from the typical use case of automating day-to-day tasks and managing multiple systems, Salt can play a part in the DevOps space as well. It can automate everything a developer requires, 
from managing source code and configuration, scheduling jobs, to cloud provisioning. Salt is vendor agnostic and highly pluggable, it is able to interface with other CI tools and version-control systems.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;1017&quot; alt=&quot;Jenkins with Salt&quot; src=&quot;https://user-images.githubusercontent.com/25560159/114967410-fbba9180-9ea6-11eb-87ec-3a467a77c9eb.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example is to integrate Jenkins with Salt. Jenkins is one of the popular CI/CD tools which most of the organisations are using, and Salt can help to make Jenkins pipeline better.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;1114&quot; alt=&quot;Jenkins Pipeline with Salt&quot; src=&quot;https://user-images.githubusercontent.com/25560159/114970156-7508b300-9eac-11eb-9d48-527fbbb5af3b.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For instance, Salt can be used to deploy and configure environment such as spawning an instance on-premise or cloud and deploying application on it, and Jenkins will execute the tests automatically upon the new Salt change. 
In such scenario, Salt also provides security to DevOps as it allows organisations to provide public and private keys for all masters and minions, guaranteeing their authenticity and providing a secure channel for master/minion communications without sacrificing performance or scalability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Event-Driven Automation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Salt is built around event-driven infrastructure, you can monitor events with Salt Runner, and non-Salt related events (system load, service status, etc) with Salt Beacon, and Salt Reactor to trigger actions in response to any event.&lt;/p&gt;

&lt;p&gt;An example is to watch for computing resources such as disk, processor, ram, and other system stats and take action automatically with pre-defined modules if it exceeds defined thresholds.&lt;/p&gt;

&lt;h2 id=&quot;vmwares-acquisition---saltstack-config-and-saltstack-secops&quot;&gt;VMware’s Acquisition - SaltStack Config and SaltStack SecOps&lt;/h2&gt;
&lt;p&gt;With VMware’s announcement of the acquisition, Salt is integrated into the vRealize Automation (vRA) as features known as SaltStack Config and SaltStack SecOps. Salt further expands
vRA capabilities into an enterprise-grade management tool which includes policy and governance, Infrastructure as Code and DevSecOps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/114902820-c5e6c000-9e48-11eb-92fb-da7cde44abb6.png&quot; alt=&quot;vRealize Automation Features&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The capabilities are summed up as below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SaltStack Config&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simple and modern languages python and YAML to manage applications and infrastructure&lt;/li&gt;
  &lt;li&gt;Self-healing with event-driven automation to detect events such as errors and apply solution automatically&lt;/li&gt;
  &lt;li&gt;Agent or agentless management of systems&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;SaltStack SecOps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Enforce continuous compliance with pre-built CIS and DISA STIGs content&lt;/li&gt;
  &lt;li&gt;Scan and remediate critical vulnerabilities with automation&lt;/li&gt;
  &lt;li&gt;Real-time insights of the state of systems and applications&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;SaltStack is pretty well known when it comes to automation and its unique capabilities. With SaltStack integrated into vRA, there is no doubt that vRA can provide
a more complete solution when it comes to event-driven automation and compliance management within the organisation’s operations, and expanding into the DevSecOps use case.&lt;/p&gt;

&lt;p&gt;You can read more about the vRA with SaltStack:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.vmware.com/management/2020/11/vmware-vrealize-automation-saltstack-config-launch.html&quot;&gt;VMware vRealize Automation Adds Native Configuration Management with vRealize Automation SaltStack Config&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.vmware.com/management/2021/04/vra-saltstack-config-secops-compliance-management.html&quot;&gt;VMware vRealize Automation SaltStack SecOps : Technical Overview Part 1 – Compliance Management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And some good SaltStack references:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://saltproject.io/infrastructure-automation-2020-a-quick-guide/&quot;&gt;A Guide to Infrastructure Automation in 2020&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/saltstack/salt&quot;&gt;SaltStack GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="automation" /><category term="devsops" /><category term="devsecops" /><category term="vmware" /><summary type="html">Have you added “Salt” to your environment? Not the condiment, salt. I meant SaltStack’s Salt…</summary></entry><entry><title type="html">Introduction to Dev + Sec + Ops</title><link href="http://localhost:4000/2021/03/04/intro-to-devsecops" rel="alternate" type="text/html" title="Introduction to Dev + Sec + Ops" /><published>2021-03-04T00:00:00+08:00</published><updated>2021-03-04T00:00:00+08:00</updated><id>http://localhost:4000/2021/03/04/What-Is-DevSecOps</id><content type="html" xml:base="http://localhost:4000/2021/03/04/intro-to-devsecops">&lt;h2 id=&quot;what-is-devsecops&quot;&gt;What is DevSecOps&lt;/h2&gt;
&lt;p&gt;DevSecOps builds on top of the DevOps methodology where it addresses code quality and reliability assurance. You probably would have already guessed it, “DevSecOps” expands “DevOps” and has an additional focus, &lt;strong&gt;security&lt;/strong&gt;; eliminating silos between development, operations and security.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/109853707-2b179500-7c91-11eb-9d0e-6bdaf16967b5.png&quot; alt=&quot;DevSecOps&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://sevaa.com/app/uploads/2018/10/ft-devsecops.png&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition of DevSecOps&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“DevSecOps is the integration of security into emerging agile IT and DevOps development as seamlessly and as transparently as possible. Ideally, this is done without reducing the agility or speed of developers or requiring them to leave their development toolchain environment.” - Gartner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Security has been in the spotlight for the past couple of years where software vulnerabilities are being exploited at both the application and infrastructure layers; on-premise and on cloud. One of such attacks was the SolarWinds hack in 2020. Over the years,
organisations have constantly transforming and changing the way they deploy applications and where they deploy them. With things moving so fast, organisations are looking for ways to tackle security challenges. 
Fundamentally with DevSecOps, security is baked into the end-to-end software development lifecycle (SDLC), and vulnerabilities can be caught and fixed before anyone can exploit them.&lt;/p&gt;

&lt;p&gt;In this article, I will be sharing some of the practices around DevSecOps and some tools that can be integrated into the pipeline. This is an expansion to my
previous article, &lt;a href=&quot;https://jjblog.io/2020/10/14/intro-to-devops&quot;&gt;Introduction to DevOps&lt;/a&gt; which covers the basics of DevOps.&lt;/p&gt;

&lt;h2 id=&quot;benefits-of-devsecops&quot;&gt;Benefits of DevSecOps&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/109900151-db0cf280-7cd1-11eb-86ca-423e3ef79796.png&quot; alt=&quot;Detect Vulnerabilities&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://www.tenable.com/sites/drupal.dmz.tenablesecurity.com/files/styles/640x360/public/images/articles/shadow_brokers_vulnerability_detection_v1.png?itok=akRCxXFk&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Adoption of DevSecOps has the following main benefits:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Early detection of vulnerabilities&lt;/strong&gt; - spot bugs at an earlier stage with an automated workflow embed with security tools.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve application overall security&lt;/strong&gt; - with security built into the application, it will have lesser bugs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Save costs on resource management&lt;/strong&gt; - risk and vulnerability assessment can be done from the start and through development instead of waiting until after a release.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-implement--transiting-to-devsecops&quot;&gt;How to implement / transiting to DevSecOps&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/109900515-769e6300-7cd2-11eb-85bc-5fbfaff73a66.png&quot; alt=&quot;DevSecOps&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://www.plutora.com/wp-content/uploads/2019/03/DevSecOps-Diagram.png&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;To integrate security into DevOps successfully, the concept of &lt;strong&gt;building security into the product&lt;/strong&gt; must be applied. It is never about having security integrated to a finished product.&lt;/p&gt;

&lt;p&gt;DevSecOps adopts a few practices in its life cycle (simplified with examples):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Culture&lt;/strong&gt; - security has to be prioritised at the beginning (top-down approach), usually starting from the planning and designing stage. Every team needs to understand their security responsibilities and how to manage risks/threats.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt; - prioritizing the security requirements as a part of the product backlog, instill security knowledge into developers, they need to take ownership and practise secure coding, writing clean codes which adhered to secured coding standards.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automation&lt;/strong&gt; - automate development and operations side of things which include applying/provisioning of repeatable security best practices through the orchestration of containers (Kubernetes),
integrate security standards and guidelines into IDEs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt; - automated security, penetration, functional and performance tests. Define your own static application security testing (SAST), dynamic application security testing (DAST), runtime application self-protection (RASP); find the right tools for these tests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt; - scanning of container images for vulnerabilities and automated deployment through secured CI/CD and APIs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Measurement and Monitoring&lt;/strong&gt; - Real-time and continuous monitoring on production workloads with alerts and notifications; and intrusion detection. Gather feedback to improve the next release of the application.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adoption of DevSecOps will have some challenges because it is more of a culture to technology. DevSecOps should start with a culture shift and getting buy-in from top management should help to drive the culture within the organisation.
To effectively drive adoption, here are some suggestions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Evaluate current security measures and conclude on how to improve them&lt;/li&gt;
  &lt;li&gt;Mandate security at every stage of the pipeline&lt;/li&gt;
  &lt;li&gt;Processes should be automated as much as possible&lt;/li&gt;
  &lt;li&gt;Developers to be trained to code securely&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tools-that-can-be-integrated-into-the-pipeline&quot;&gt;Tools that can be integrated into the pipeline&lt;/h2&gt;
&lt;p&gt;There are many tools available, both commercial or open source, that can be integrated into existing CI/CD pipeline and below are some of my suggestions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SonarQube&lt;/strong&gt; - continuous inspection of code quality to perform automatic reviews with static analysis of code.&lt;br /&gt;
&lt;strong&gt;Harbor&lt;/strong&gt; - open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted.&lt;br /&gt;
&lt;strong&gt;HashiCorp Vault&lt;/strong&gt; - secures, stores, and tightly controls access to tokens, passwords, certificates, API keys, and other secrets in modern computing.&lt;br /&gt;
&lt;strong&gt;ThreatModeler&lt;/strong&gt; - identify, predict and define threats across the entire attack surface to make proactive security decisions and minimize overall risk. &lt;br /&gt;
&lt;strong&gt;Aqua Security&lt;/strong&gt; - provides security for containers, serverless and cloud-native applications throughout the DevSecOps pipeline.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;DevSecOps builds trust and security into an application. As organisations are undergoing digital transformation, moving into the cloud native world with containers and microservices, it introduces new 
things to worry about and security remains prominent. There will never be a fully secured application but fostering the DevSecOps mindset and culture will be the key to tackle security challenges in the unfamiliar environment.
Like many security programmes, the successful implementation is heavily dependent on people, processes and technologies; everyone has a role to play.&lt;/p&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="devsecops" /><category term="appdev" /><summary type="html">What is DevSecOps DevSecOps builds on top of the DevOps methodology where it addresses code quality and reliability assurance. You probably would have already guessed it, “DevSecOps” expands “DevOps” and has an additional focus, security; eliminating silos between development, operations and security.</summary></entry><entry><title type="html">Introduction to Dev + Ops</title><link href="http://localhost:4000/2020/10/14/intro-to-devops" rel="alternate" type="text/html" title="Introduction to Dev + Ops" /><published>2020-10-14T00:00:00+08:00</published><updated>2020-10-14T00:00:00+08:00</updated><id>http://localhost:4000/2020/10/14/Dev+Ops</id><content type="html" xml:base="http://localhost:4000/2020/10/14/intro-to-devops">&lt;p&gt;The word “DevOps” is a common buzzword which is familiar and of course it does mean different things to many people. Hence in this post, I will be sharing 
the fundamental concepts of DevOps together with some common tools that companies are using and a simple setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95935658-54c94000-0e06-11eb-8774-03e10cea6aa2.png&quot; alt=&quot;DevOps&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-devops&quot;&gt;What is DevOps&lt;/h2&gt;
&lt;p&gt;DevOps comes from the two words “development” and “operations”, and it is all about combining the power of 
developers and operations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition of DevOps&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DevOps is a process framework which fosters agility in software development, and collaboration between development and operations team to deploy applications faster
in a repeatable and automated manner. With DevOps, businesses can expect faster time to market, increased productivity and revenue, business agility and many more.
Also, DevOps comprises a set of principles for software delivery and is a cultural change which involves collaboration of cross-functional teams which includes 
development, operations, security, test and business.&lt;/p&gt;

&lt;h2 id=&quot;challenges-faced-before-devops&quot;&gt;Challenges faced before DevOps&lt;/h2&gt;
&lt;p&gt;Below are some of the typical challenges faced by the development and operations team before DevOps process framework is in placed:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How to manage the demand for resources when it comes to scaling&lt;/li&gt;
  &lt;li&gt;How to handle tweaks in an application in a production environment&lt;/li&gt;
  &lt;li&gt;Long release cycle requires more time to troubleshoot and resolve issues that occur during deployment in production environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The misalignment and lack of collaboration between the two departments are often the culprit for the challenges. For instance when it comes to scaling,
how can the application be deployed when lots of time is needed to prepare the infrastructure? And when it comes to addressing deployment error,
how to ensure that there is zero to minimum downtime, and the rolling back to the previous version can be done immediately?&lt;/p&gt;

&lt;h2 id=&quot;how-devops-addresses-these-challenges&quot;&gt;How DevOps addresses these challenges&lt;/h2&gt;
&lt;p&gt;DevOps follows a modular approach towards software development with a clear plan on how to break down existing monolithic applications into microservices. 
Instead of having big releases of application features, DevOps allows the roll out of bite-size features to their customers through a series of release iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95830684-a6bc8800-0d6a-11eb-948e-732b5264b674.png&quot; alt=&quot;DevOps life cycle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DevOps adopts a few practices in its life cycle (simplified):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Continuous planning&lt;/strong&gt; - identifying resources and priorities&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collaborative development&lt;/strong&gt; - frequent code integrations and automatic build&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous testing&lt;/strong&gt; - test application, find defects, rebuild if necessary&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous release and deployment&lt;/strong&gt; - application goes Live if it passes the testing phase&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous monitoring&lt;/strong&gt; - monitor performance as per requirements, early feedback helps to steer projects in the right direction&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous feedback and optimization&lt;/strong&gt; - provide visibility on root cause which impacts business&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cicd-pipeline&quot;&gt;CI/CD pipeline&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95941255-e9399f80-0e12-11eb-8b5b-6e95e6686b72.png&quot; alt=&quot;CICD&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://www.redhat.com/cms/managed-files/ci-cd-flow-desktop_1.png&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition of continuous integration (CI)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Continuous integration (CI) is the practice of merging all developers’ working copies to a shared mainline several times a day” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Definition of continuous delivery (CD)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Continuous delivery (CD) is a software engineering approach in which teams produce software in short cycles, ensuring that the software can be reliably released at any time and, when releasing the software, doing so manually” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Definition of continuous deployment (CI)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Continuous deployment (CD) is a software engineering approach in which software functionalities are delivered frequently through automated deployments” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To implement DevOps process framework, a CI/CD pipeline needs to be created in an automated and repeatable way:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95937080-88599980-0e09-11eb-98b9-896599d31577.png&quot; alt=&quot;Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A CI/CD pipeline serves as the backbone of the DevOps process framework. It comprises a set of tools from conceptualize to delivery of applications from the planning to production phase.
As mentioned above, DevOps is a cross-functional discipline, multiple tools are needed to cover the variety of stages, personas and activities. The set of tools in the pipeline are integrated
and the flow of code through the different phases is automated to avoid human error and delays.&lt;/p&gt;

&lt;h2 id=&quot;devops-tools-as-per-life-cycle&quot;&gt;DevOps Tools (as per life cycle)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95940452-f48bcb80-0e10-11eb-88cc-4fc404a8e3db.png&quot; alt=&quot;DevOps Tools&quot; /&gt;
&lt;sub&gt;&lt;sup&gt;&lt;a href=&quot;https://ncplinc.com/includes/images/blog/ncpl-open-source-devops-tools.png&quot;&gt;Image Source&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Below are some suggestions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Planning&lt;/strong&gt; - Trello, Kanboard, Jira, Slack&lt;br /&gt;
&lt;strong&gt;Code&lt;/strong&gt; - Git, SVN, Bitbucket, Eclipse Che&lt;br /&gt;
&lt;strong&gt;Build&lt;/strong&gt; - Maven, Gradle, Packer&lt;br /&gt;
&lt;strong&gt;Test&lt;/strong&gt; - Apache JMeter, JUnit, Selenium &lt;br /&gt;
&lt;strong&gt;Release, Deploy, Operate&lt;/strong&gt; - Kubernetes, Ansible, Terraform, Go, Docker&lt;br /&gt;
&lt;strong&gt;Monitoring&lt;/strong&gt; - Grafana, Prometheus, Fluentd, Nagios, Splunk, Istio, Elastic&lt;br /&gt;
&lt;strong&gt;Feedback&lt;/strong&gt; - Slack, Jira&lt;/p&gt;

&lt;p&gt;Note that there are tons of open source tools available to help businesses to build their pipeline where businesses can evaluate based on their requirements.&lt;/p&gt;

&lt;h2 id=&quot;simple-setup---devops-with-openshift-red-hats-flavour-of-kubernetes&quot;&gt;Simple Setup - DevOps with OpenShift (Red Hat’s flavour of Kubernetes)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/95900543-a4871780-0dc4-11eb-84a1-a27c93af3421.png&quot; alt=&quot;DevOps with OpenShift&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main components of the setup include OpenShift and Jenkins.
OpenShift is the container orchestration platform to orchestrate the containers (microservices), with a Jenkins pipeline running outside of OpenShift clusters to
facilitate the deployment of applications across different environments (Dev, Test, UAT and PROD). Application testings should be conducted before deployment to each environment. For example, system and smoke test are done in DEV, 
while functional and security tests are done in TEST.&lt;/p&gt;

&lt;p&gt;The pipeline starts when developers commit their codes to the master branch.
The whole pipeline is automated and repeatable. Should the application deployment fails at certain stage of the pipeline, the current Live
application is not affected and developers can continue to make changes and commit the codes again to trigger the pipeline. A release manager is also in placed
to approve the deployment to PROD manually (As PROD is serving Live traffic, it usually requires manual approval).&lt;/p&gt;

&lt;p&gt;DevOps with OpenShift is a cloud native approach aligned with a hybrid and multi cloud strategy. OpenShift provides
standardized environment for developers regardless of underlying infrastructure (AWS, Microsoft Azure, Google Cloud, etc). As such, businesses can truly enjoy application mobility and portability across 
different infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;DevOps adoption is crucial to the success of a digital transformation journey. 
It enables developers and operations to work closely as one unit throughout all the phases of an application development.
DevOps is more than just a project, it is a cultural change which requires companies to break away from conventional mindset.
To better understand DevOps, I strongly recommend a novel, The Phoenix Project, it talks about transformation journey and illustrates common problems which are similar to real life.&lt;/p&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="redhat" /><category term="devops" /><category term="appdev" /><category term="openshift" /><summary type="html">The word “DevOps” is a common buzzword which is familiar and of course it does mean different things to many people. Hence in this post, I will be sharing the fundamental concepts of DevOps together with some common tools that companies are using and a simple setup.</summary></entry><entry><title type="html">Externalize Your Business Rules With Red Hat Decision Manager</title><link href="http://localhost:4000/2020/08/12/externalize-your-business-rules" rel="alternate" type="text/html" title="Externalize Your Business Rules With Red Hat Decision Manager" /><published>2020-08-12T00:00:00+08:00</published><updated>2020-08-12T00:00:00+08:00</updated><id>http://localhost:4000/2020/08/12/Externalize-Your-Business-Rules</id><content type="html" xml:base="http://localhost:4000/2020/08/12/externalize-your-business-rules">&lt;p&gt;In this post, I will be sharing my experience on using Red Hat Decision Manager (RHDM) as the rules engine, using DRL (Drools Rule Language) rules to isolate decision logic from application 
code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/90093092-4d7dab80-dd5d-11ea-842d-2839045814cb.png&quot; alt=&quot;Red Hat Decision Manager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recently, I was involved in creating a fraudulent detection demo/workshop which requires decision making based on a certain set of rules. 
The part which involves business rules is when a prediction model computes a score from the raw data received, and the application needs to make a 
decision based on the score. This is the part where a rules engine manages decision processes using pre-defined logic to determine outcomes.&lt;/p&gt;

&lt;p&gt;Before going into my setup, let’s understand why there is a need for a business rules engine. Usually, business rules are hidden inside an application code and this is a problem for non-technical
folks. Non-technical folks like the business people know the rules and regulations policy but do not understand application code. It will be difficult to manage the rules especially when 
business applications can contain tens of thousands of rules which are continually changing. Therefore, there will be impact when it comes to adding, removing or changing rules.
As such, an external business rules engine helps to decouple application code from business rules and allows a centralized management of rules.&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts&lt;/h2&gt;
&lt;p&gt;RHDM is based on the &lt;a href=&quot;https://www.drools.org/&quot;&gt;Drools&lt;/a&gt; project, one of the widest used rules engines in the industry which many in-house applications are already using.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/90094510-b74b8480-dd60-11ea-91cf-5f5ed519f2e3.png&quot; alt=&quot;Drools Engine&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The decision engine operates using the following basic components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Rules&lt;/strong&gt;: Business rules or DMN decisions that you define. All rules must contain at a minimum the conditions that trigger the rule and the actions that the rule dictates.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Facts&lt;/strong&gt;: Data that enters or changes in the decision engine that the decision engine matches to rule conditions to execute applicable rules.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Production memory&lt;/strong&gt;: Location where rules are stored in the decision engine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Working memory&lt;/strong&gt;: Location where facts are stored in the decision engine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Agenda&lt;/strong&gt;: Location where activated rules are registered and sorted (if applicable) in preparation for execution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What is a rules engine&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“A business rules engine is a software system that executes one or more business rules in a runtime production environment” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A rules engine is a collection of objects with conditions and actions. Businesses can run their process model against it and let it run through the conditions.&lt;/p&gt;

&lt;p&gt;An example would be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Company’s credit score of 50 or less will be given the status of “Disallow”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are a few benefits of using a rules engine:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Logical separation of rules from application code.&lt;/li&gt;
  &lt;li&gt;Human-readable rule which allows non-technical folks to understand the rules easily.&lt;/li&gt;
  &lt;li&gt;Centralization of rules provides flexibility to make adjustment easily without the need of going through the application codes.&lt;/li&gt;
  &lt;li&gt;Rules can be reused.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What is a business rule&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“A business rule defines or constrains some aspect of business and always resolves to either true or false. It specifically involves terms, facts and rules. 
Business rules are intended to assert business structure or to control or influence the behavior of the business.” - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Business rules are essential to guide and help in decision making. Businesses would aim to automate them but not every business decision can be automated. The goal is to
automate repeatable operational decisions.&lt;/p&gt;

&lt;p&gt;An example would be something like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A bank would disapprove a loan to a customer/company which has a credit rating of 50 or lower.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;set-up&quot;&gt;Set up&lt;/h2&gt;
&lt;p&gt;There are many ways to set up RHDM and here is how I set it up:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/90087001-4d29e400-dd4e-11ea-8501-3dc3ebe5eaf3.png&quot; alt=&quot;RHDM Arch&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Decision Central - An authoring environment with GUI to allow low code development (DMN, DRL rules, decision table, etc). It also manages Kjars deployments to multiple Kie Servers.&lt;/li&gt;
  &lt;li&gt;KIE Server - It hosts the decision services which execute rules and processes. KIE server exposes its functionality via REST, JMS and Java interfaces to client applications.&lt;/li&gt;
  &lt;li&gt;Git - It stores business assets such as DMN model, decision tables, etc. In my case, it stores the DRL rules.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My RHDM sits on OpenShift Container Platform where my KIE server runs in a container and is able to leverage on Kubernetes’s capabilities (self-healing, etc). 
Whenever I update and commit my rules to the Git server, it automatically triggers the CI/CD flow to deploy the new set of rules embedded into the KIE server, and roll out the
container deployment using a rolling update strategy where the previous instance is terminated only when the new one is ready.&lt;/p&gt;

&lt;p&gt;Here is how I write my rules in DRL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/90090035-8ca7fe80-dd55-11ea-996f-611548b7477b.png&quot; alt=&quot;DRL&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I used the Decision Central to create a Result object and write my DRL rules. My DRL rules are simple, I am bascially writing a condition where a score of less than 70% would mean not fraud and a score of more than 70% means fraud. 
Afterwhich, I built and deployed my KIE server with the DRL rules. When the KIE server is deployed successfully, my application sends a payload with the score and receives a response payload containing the status.&lt;/p&gt;

&lt;p&gt;My written rules can be found on my Git Hub, &lt;a href=&quot;https://github.com/jiajunngjj/drl-fraud-demo&quot;&gt;https://github.com/jiajunngjj/drl-fraud-demo&lt;/a&gt; which can be imported directly onto Decision Central as well.
There are many ways to create your decision service and since my project is just a simple set of rules, DRL rules are sufficient.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It is always the best practice to externalize rules from applications to allow a more agile and efficient development. 
There are many use cases which RHDM can be part of the solution from fraud detection in banking industry to automated trading in capital markets and etc.&lt;/p&gt;

&lt;p&gt;While driving applications with external business rules is beneficial, 
it is also important to note that it is an additional layer to an application architecture, hence the application and its requirements should be carefully designed to ensure it can leverage on the external engine with
minimal to zero application code change when a rule changes.&lt;/p&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="redhat" /><category term="decision manager" /><category term="drools" /><category term="jboss" /><category term="appdev" /><summary type="html">In this post, I will be sharing my experience on using Red Hat Decision Manager (RHDM) as the rules engine, using DRL (Drools Rule Language) rules to isolate decision logic from application code.</summary></entry><entry><title type="html">Quarkus, a lightweight Java for Serverless</title><link href="http://localhost:4000/2020/05/11/quarkus-openshift-serverless" rel="alternate" type="text/html" title="Quarkus, a lightweight Java for Serverless" /><published>2020-05-11T00:00:00+08:00</published><updated>2020-05-11T00:00:00+08:00</updated><id>http://localhost:4000/2020/05/11/Quarkus-Openshift-Serverless</id><content type="html" xml:base="http://localhost:4000/2020/05/11/quarkus-openshift-serverless">&lt;p&gt;Java has been around for more than 20 years and is still popular among developers today. However, with the digital landscape moving towards cloud, Java
is often seems as an unfavourable option because of its heavy-duty performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/81423897-ad295100-9187-11ea-8baf-94e0e232fb88.png&quot; alt=&quot;Quarkus Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, fear not. Quarkus is here to bring Java into the cloud-native world. Quarkus is an open source, full stack, Kubernetes-native Java framework which optimizes Java for
the cloud future. Quarkus promises:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fast startup (~0.055 Seconds for REST + CRUD)&lt;/li&gt;
  &lt;li&gt;Low memory (~35 MB for REST + CRUD)&lt;/li&gt;
  &lt;li&gt;Small footprint (higher density)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/81463365-b3511900-91eb-11ea-9d14-f549c45b1a8f.png&quot; alt=&quot;Quarkus_metrics from http://quarkus.io/&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hence it is very suitable for containers and serverless. For instance, when running a simple hello world Quarkus application, it starts in ~1s:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/81528510-76be2280-938f-11ea-9948-e4ea099e6ad6.png&quot; alt=&quot;command line output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This simple hello world Quarkus application can found on my &lt;a href=&quot;https://github.com/jiajunngjj/quarkus-helloworld&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Quarkus works out-of-the-box with popular Java standards, frameworks, and libraries like Eclipse MicroProfile and Vert.x. Quarkus dependency injection is based 
on CDI hence Java developers are able to use JAX-RS, JPA and many others. Quarkus also includes an extension framework which allows 
developers to write extensions for integration and enhancement. A list of existing available extensions can be found &lt;a href=&quot;https://github.com/quarkusio/quarkus/tree/master/extensions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Run-time wise, Quarkus offers two run modes:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;JVM&lt;/li&gt;
  &lt;li&gt;Native&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;JVM mode packages Quarkus applications as JAR files and run on vanilla OpenJDK HotSpot while native mode uses GraalVM to create a standalone executable that does not
run in a Java VM. Both modes have its pros and cons, and it depends on use case to see which mode is the best fit. Considering an event-driven scenario where events trigger the function 
to spin up a service in real-time to react to the event. It will make sense to use native mode because JVM takes a while to start.&lt;/p&gt;

&lt;p&gt;An interesting feature is the hot-reload capability (dev mode) where any changes made to any Java file, application config or static 
resources will be compiled automatically once the browser is refreshed. This is made possible with Quarkus augmentation phase where almost everything (metadata, 
config parsing, logics) is already configured in build-time, the whole reload takes less than a second when changes are made in run-time.  This is enabled by default and to use it, just need to run:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn compile quarkus:dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This improves developer’s experience especially when it comes to debugging.&lt;/p&gt;

&lt;h2 id=&quot;serverless-architecture&quot;&gt;Serverless Architecture&lt;/h2&gt;
&lt;p&gt;As Quarkus has extremely short startup time, it brings Java developers to the serverless world where workloads can be scaled or scaled to zero based on demand. 
Quarkus is a good fit for serverless applications and is event-driven in nature. Together with Kubernetes, the serverless pattern can be implemented easily where
an event triggers the application and Kubernetes starts a container to handle that event. The application might produce some results from that compute or 
processing and once idle for enough time, that container will be scaled down to zero. This solves the under-provisioning and over-provisioning problems. I’ve written
a demo using Quarkus and Knative to create a serverless application on OpenShift (Red Hat’s flavour of Kubernetes). OpenShift Serverless is based on Knative, an open 
source Kubernetes serverless project, which is generally available today.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/81521596-40c27380-937a-11ea-9e3d-bb3ec9a048b1.png&quot; alt=&quot;Serverless Pattern&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Demo: &lt;a href=&quot;https://github.com/jiajunngjj/knative-quarkus-openshift-demo&quot;&gt;https://github.com/jiajunngjj/knative-quarkus-openshift-demo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;All in all, Quarkus aims to optimize Java development for distributed application architectures. Its container-first approach is aligned with microservices and event-driven
architecture, and digital transformation strategies. Also, serverless computing model plays an important role in the cloud because it controls computing cost.
Quarkus in Java ecosystem seems to be a game-changer, and moving forward, I’m excited to see how Quarkus can bring fast innovation and help enterprises to stay ahead of competitors.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://quarkus.io/&quot;&gt;http://quarkus.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/quarkusio/quarkus&quot;&gt;https://github.com/quarkusio/quarkus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="redhat" /><category term="openshift" /><category term="serverless" /><category term="quarkus" /><category term="java" /><category term="event-driven" /><category term="knative" /><summary type="html">Java has been around for more than 20 years and is still popular among developers today. However, with the digital landscape moving towards cloud, Java is often seems as an unfavourable option because of its heavy-duty performance.</summary></entry><entry><title type="html">AMQ Messaging on OpenShift</title><link href="http://localhost:4000/2020/04/27/amq-openshift" rel="alternate" type="text/html" title="AMQ Messaging on OpenShift" /><published>2020-04-27T00:00:00+08:00</published><updated>2020-04-27T00:00:00+08:00</updated><id>http://localhost:4000/2020/04/27/AMQ-Cluster-On-OpenShift</id><content type="html" xml:base="http://localhost:4000/2020/04/27/amq-openshift">&lt;p&gt;More companies are moving towards a microservices approach for their new applications.
This approach suits today’s increasingly distributed hybrid cloud and multi-cloud IT infrastructures. Thanks to advances in container orchestration platform, it is easier to 
edge away from traditional monolithic applications towards distributed and highly-available microservices. Microservices approach would mean more services and there is a need for
 a messaging layer to serve the inter-service communication. In this post, I’m going to share how AMQ brokers can leverage on OpenShift.&lt;/p&gt;

&lt;h2 id=&quot;topology&quot;&gt;Topology&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/80358496-1ae39c00-88af-11ea-8607-421b9d1a9a3b.png&quot; alt=&quot;Red Hat - AMQ Broker on OpenShift&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unlike traditional deployment of AMQ broker where configuration is needed for high availability (HA) and clustering, AMQ brokers on OpenShift form a cluster automatically when there is more than 
one broker Pod. Note that there is no master/slave failover model on OpenShift so there is no need for message replication between master/slave either.&lt;/p&gt;

&lt;p&gt;When determining the number of  brokers required to handle certain workload, OpenShift has auto scaling feature which can be configured based on CPU consumption. For instance, a threshold can be set to 70% and when the CPU utilization reaches 70%, 
OpenShift scales up the AMQ broker Pods; and it scales down when the heavy traffic is gone.&lt;/p&gt;

&lt;p&gt;Internal services connecting to the broker within Openshift cluster is straightforward through the Kubernetes concept of Services. However, external services running outside of OpenShift will have some restrictions to connect to the broker.
One easiest option is to use SSL and access the broker from the route, and the other is through NodePort binding.&lt;/p&gt;

&lt;p&gt;Demo: &lt;a href=&quot;https://github.com/jiajunng/amq-openshift-demo&quot;&gt;https://github.com/jiajunngjj/amq-openshift-demo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;high-availability-ha&quot;&gt;High Availability (HA)&lt;/h2&gt;
&lt;p&gt;The availability of the brokers and integrity of the messaging data are maintained through Kubernetes concepts of Health Checks, Stateful Sets, Persistent Volumes (PVs) and Persistent Volume Claims (PVCs).
Health checks are configured in OpenShift to detect and restart unhealthy containers. In the HA configuration of AMQ brokers on OpenShift, there will be at least 2 broker Pods running with each writing messaging data to its own PV. If a broker Pod goes offline, the message
data stored in that PV is redistributed to an alternative available broker, which then stores it in its own PV.&lt;/p&gt;

&lt;p&gt;Message migration can be enabled on OpenShift to migrate messages when scaling down broker Pods due to failure. This further protects the integrity of messaging data.&lt;/p&gt;

&lt;p&gt;Demo: &lt;a href=&quot;https://github.com/jiajunng/amq-ha-openshift-demo&quot;&gt;https://github.com/jiajunngjj/amq-ha-openshift-demo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;storage&quot;&gt;Storage&lt;/h2&gt;
&lt;p&gt;With container-native storage (OpenShift Container Storage) deployed on OpenShift, PV can be provisioned dynamically with the broker Pod. Else, PV has to be manually provisioned and ensure that they are available 
for broker Pod to consume. For instance, a cluster of two broker Pods with persistent storage would required two PVs available. Note that in OpenShift deployment, there is no need for a
shared file system with distributed locking capabilities since each broker Pod has their own PV and message data is redistributed if it goes offline.&lt;/p&gt;

&lt;h2 id=&quot;multi-cluster&quot;&gt;Multi-Cluster&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25560159/80358992-cc82cd00-88af-11ea-82eb-bd50300ea1c3.png&quot; alt=&quot;AMQ Broker - Multi-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With more companies leaning towards hybrid and multi-cloud strategy, it is common to have more than one OpenShift clusters across different data centers. 
AMQ Interconnect is the solution to connect brokers across OpenShift clusters. It is a lightweight AMQP message router which routes messaging data 
between AMQP-enabled endpoints, including clients, brokers, and standalone services. 
AMQ Interconnect is based on Dispatch Router from the Apache Qpid™ project.&lt;/p&gt;

&lt;p&gt;Demo: &lt;a href=&quot;https://github.com/jiajunng/amq-federation-openshift-demo&quot;&gt;https://github.com/jiajunngjj/amq-federation-openshift-demo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;There are many benefits of deploying AMQ broker on OpenShift as AMQ provides simple configuration for allowing the setup of a Broker Mesh, 
from how configurations are made easier to leveraging on Kubernetes-native capabilities. It is something worth considering when it comes to microservices
strategy, which is also to have middleware workload containerized and deployed on OpenShift.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/deploying_amq_broker_on_openshift/index&quot;&gt;Red Hat AMQ Broker On OpenShift Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/deploying_amq_interconnect_on_openshift/index&quot;&gt;Red Hat AMQ Interconnect On OpenShift Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="redhat" /><category term="openshift" /><category term="amq broker" /><category term="messaging" /><category term="event-driven" /><summary type="html">More companies are moving towards a microservices approach for their new applications. This approach suits today’s increasingly distributed hybrid cloud and multi-cloud IT infrastructures. Thanks to advances in container orchestration platform, it is easier to edge away from traditional monolithic applications towards distributed and highly-available microservices. Microservices approach would mean more services and there is a need for a messaging layer to serve the inter-service communication. In this post, I’m going to share how AMQ brokers can leverage on OpenShift.</summary></entry><entry><title type="html">Configure HostPath Storage For Prometheus on OpenShift v3.11</title><link href="http://localhost:4000/2019/04/19/prometheus-openshift" rel="alternate" type="text/html" title="Configure HostPath Storage For Prometheus on OpenShift v3.11" /><published>2019-04-19T00:00:00+08:00</published><updated>2019-04-19T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/19/Configure-HostPath-Storage-For-Prometheus-On-OpenShift-v3.11</id><content type="html" xml:base="http://localhost:4000/2019/04/19/prometheus-openshift">&lt;p&gt;This post covers the configuration of persistent storage with HostPath for OpenShift Monitoring stack.&lt;/p&gt;

&lt;p&gt;In OpenShift v3.11, Prometheus cluster monitoring is now fully supported and deployed by default. As the monitoring stack is fairly new, there are many on-going discussions on supported storage plugins. And the documentation does not cover the detailed steps of setting up the storage. The configurability is also limited with Operator in placed which “protects” the configuration.&lt;/p&gt;

&lt;p&gt;Through various implementations with HostPath requirement, I have figured a way to make HostPath storage works with Operator.&lt;/p&gt;

&lt;h2 id=&quot;configuration-steps&quot;&gt;Configuration Steps&lt;/h2&gt;
&lt;h3 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;HostPath SCC is already created&lt;/li&gt;
  &lt;li&gt;HostPath SCC set to allow hostpath directory volume plugin&lt;/li&gt;
  &lt;li&gt;Granting access to this SCC to all users&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inventory-file&quot;&gt;Inventory file&lt;/h3&gt;
&lt;p&gt;It is mandatory set the following variables in the inventory file:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;openshift_cluster_monitoring_operator_node_selector={‘role’:’metrics’}&lt;/td&gt;
      &lt;td&gt;I have created a label, “role=metrics” to land the pods on the dedicated nodes for Prometheus/Alertmanager. These nodes should contain the HostPath for persistent storage.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;openshift_cluster_monitoring_operator_prometheus_storage_capacity=50G&lt;/td&gt;
      &lt;td&gt;The persistent volume claim size for each of the Prometheus instances. This variable applies only if openshift_cluster_monitoring_operator_prometheus_storage_enabled is set to true. Defaults to 50Gi.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi&lt;/td&gt;
      &lt;td&gt;The persistent volume claim size for each of the Alertmanager instances. This variable applies only if openshift_cluster_monitoring_operator_alertmanager_storage_enabled is set to true. Defaults to 2Gi.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;openshift_cluster_monitoring_operator_prometheus_storage_enabled=true&lt;/td&gt;
      &lt;td&gt;Enable persistent storage for Prometheus&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true&lt;/td&gt;
      &lt;td&gt;Enable persistent storage for Alertmanager&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;run-the-playbook&quot;&gt;Run the playbook&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ansible-playbook -i &amp;lt;INVENTORY&amp;gt;  /usr/share/ansible/openshift-ansible/playbooks/openshift-monitoring/config.yml```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;node-hostpath-configuration&quot;&gt;Node hostpath configuration&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Find the namespace ID:
```
$ oc get namespace openshift-monitoring -o yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/description: Openshift Monitoring
    openshift.io/display-name: “”
    openshift.io/node-selector: “”
    openshift.io/sa.scc.mcs: s0:c11,c0
    openshift.io/sa.scc.supplemental-groups: 1000110000/10000
    openshift.io/sa.scc.uid-range: 1000110000/10000
  creationTimestamp: 2019-04-29T03:48:23Z
  labels:
    openshift.io/cluster-monitoring: “true”
  name: openshift-monitoring
  resourceVersion: “15977564”
  selfLink: /api/v1/namespaces/openshift-monitoring
  uid: a3173b3e-6a31-11e9-b915-000c29451a06
spec:
  finalizers:
    &lt;ul&gt;
      &lt;li&gt;kubernetes
status:
  phase: Active
```&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the example above, my namespace ID is 1000110000.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change ownership and SELinux label of the HostPath:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ chown 1000110000:root /metrics/
  $ semanage fcontext -a -t svirt_sandbox_file_t /metrics
  $ restorecon -v /metrics/
  $ chown 1000110000:root /alertmanager/
  $ semanage fcontext -a -t svirt_sandbox_file_t /alertmanager
  $ restorecon -v /alertmanager/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;post-installation---prometheus&quot;&gt;Post Installation - Prometheus&lt;/h3&gt;
&lt;p&gt;After the playbook has completed successfully, you will see that the pods are starting:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; $ oc get pods
 NAME                                          READY     STATUS    RESTARTS   AGE
 cluster-monitoring-operator-6566bf44b-sb8wk   1/1       Running   0          1m
 prometheus-operator-77bc9b6c68-gw92h          1/1       Running   0          28s
  
 NAME                                          READY     STATUS    RESTARTS   AGE
 cluster-monitoring-operator-6566bf44b-sb8wk   1/1       Running   0          2m
 grafana-67cb69b946-684vc                      2/2       Running   0          55s
 prometheus-k8s-0                              0/4       Pending   0          5s
 prometheus-operator-77bc9b6c68-gw92h          1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By default, when openshift_cluster_monitoring_operator_prometheus_storage_enabled is set to true, it will create a default storage class and Prometheus-k8s-0 will remain in Pending state until it has successfully claims its PV. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first thing to do is to delete the PVC created by deployment:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ oc get pvc
  NAME                                 STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
  prometheus-k8s-db-prometheus-k8s-0   Pending                                                      4m

  $ oc delete pvc prometheus-k8s-db-prometheus-k8s-0
  persistentvolumeclaim &quot;prometheus-k8s-db-prometheus-k8s-0&quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create the new PV for Prometheus:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ vi pv-prometheus-0.yml

  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: prometheus-k8s-db-prometheus-k8s-0
  spec:
    capacity:
      storage: 100G
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: /metrics
    persistentVolumeReclaimPolicy: Retain
  ClaimRef:
    name: prometheus-k8s-db-prometheus-k8s-0
    namespace: openshift-monitoring

  $ oc create -f pv-prometheus-0.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Note that /metrics is the HostPath attached to the node.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Create the new PVC for Prometheus:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ vi pvc-prometheus-0.yml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: prometheus-k8s-db-prometheus-k8s-0
  spec:
    resources:
      requests:
        storage: 100G
    accessModes:
      - ReadWriteOnce

  $ oc create -f pvc-prometheus-0.yml -n openshift-monitoring
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;After this is done, the second Prometheus pod will start to spin up. So, repeat the steps 1 to 3, creating a new PV and PVC with prometheus-k8s-db-prometheus-k8s-1:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ vi pv-prometheus-1.yml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: prometheus-k8s-db-prometheus-k8s-1
  spec:
    capacity:
      storage: 100G
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: /metrics
    persistentVolumeReclaimPolicy: Retain
  ClaimRef:
    name: prometheus-k8s-db-prometheus-k8s-1
    namespace: openshift-monitoring


  $ vi pvc-prometheus-1.yml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: prometheus-k8s-db-prometheus-k8s-1
  spec:
    resources:
      requests:
        storage: 100G
  accessModes:
    - ReadWriteOnce
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;post-installation--alertmanager&quot;&gt;Post Installation - Alertmanager&lt;/h3&gt;
&lt;p&gt;After two Prometheus pods are spun up successfully, Alertmanager pods will be spinning up right after.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ oc get pods
  NAME                                          READY     STATUS    RESTARTS   AGE
  alertmanager-main-0                           0/3       Pending   0          42s
  cluster-monitoring-operator-6566bf44b-sb8wk   1/1       Running   0          14m
  grafana-67cb69b946-684vc                      2/2       Running   0          13m
  prometheus-k8s-0                              4/4       Running   1          1m
  prometheus-k8s-1                              4/4       Running   3          6m
  prometheus-operator-77bc9b6c68-gw92h          1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, the alertmanager will remain at Pending status until it has claimed the PV successfully.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Delete the Alertmanager PVC created by deployment:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ oc get pvc
      NAME                                       STATUS    VOLUME                               CAPACITY   ACCESS MODES   STORAGECLASS   AGE
      alertmanager-main-db-alertmanager-main-0   Pending                                                                                 41s
      prometheus-k8s-db-prometheus-k8s-0         Bound     prometheus-k8s-db-prometheus-k8s-0   100G       RWO                           7m
      prometheus-k8s-db-prometheus-k8s-1         Bound     prometheus-k8s-db-prometheus-k8s-1   100G       RWO                           4m

 $ oc delete pvc alertmanager-main-db-alertmanager-main-0
 persistentvolumeclaim &quot;alertmanager-main-db-alertmanager-main-0&quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create the new PV for Alertmanager:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ vi pv-alertmanager-0.yml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: alertmanager-main-db-alertmanager-main-0
  spec:
    capacity:
      storage: 10G
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: /alertmanager
    persistentVolumeReclaimPolicy: Retain
  storageClassName: alertmanager-storageclass
  ClaimRef:
    name: alertmanager-main-db-alertmanager-main-0
    namespace: openshift-monitoring

  $ oc create -f pv-alertmanager-0.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Note that /alertmanager is the HostPath attached to the node.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Create the PVC for Alertmanager:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ vi pvc-alertmanager-0.yml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: alertmanager-main-db-alertmanager-main-0
  spec:
    resources:
      requests:
        storage: 10G
    accessModes:
      - ReadWriteOnce

  $ oc create -f pvc-alertmanager-0.yml -n openshift-monitoring
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After this is done, the second and third Alertmanager pod will start to spin up. So, repeat the steps 1 to 3, creating a new PV and PVC with alertmanager-main-db-alertmanager-main-1 and alertmanager-main-db-alertmanager-main-2 respectively.&lt;/p&gt;

&lt;p&gt;The end result for the PV and PVC in OpenShift-Monitoring:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ oc get pv
  NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                                       STORAGECLASS   REASON    AGE
  alertmanager-main-db-alertmanager-main-0   10G        RWO            Retain           Bound     openshift-monitoring/alertmanager-main-db-alertmanager-main-0                            5m
  alertmanager-main-db-alertmanager-main-1   10G        RWO            Retain           Bound     openshift-monitoring/alertmanager-main-db-alertmanager-main-1                            1m
  alertmanager-main-db-alertmanager-main-2   10G        RWO            Retain           Bound     openshift-monitoring/alertmanager-main-db-alertmanager-main-2                            8s
  prometheus-k8s-db-prometheus-k8s-0         100G       RWO            Retain           Bound     openshift-monitoring/prometheus-k8s-db-prometheus-k8s-0                                  13m
  prometheus-k8s-db-prometheus-k8s-1         100G       RWO            Retain           Bound     openshift-monitoring/prometheus-k8s-db-prometheus-k8s-1                                  11m

  $ oc get pvc
  NAME                                       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
  alertmanager-main-db-alertmanager-main-0   Bound     alertmanager-main-db-alertmanager-main-0   10G        RWO                           3m
  alertmanager-main-db-alertmanager-main-1   Bound     alertmanager-main-db-alertmanager-main-1   10G        RWO                           1m
  alertmanager-main-db-alertmanager-main-2   Bound     alertmanager-main-db-alertmanager-main-2   10G        RWO                           6s
  prometheus-k8s-db-prometheus-k8s-0         Bound     prometheus-k8s-db-prometheus-k8s-0         100G       RWO                           13m
  prometheus-k8s-db-prometheus-k8s-1         Bound     prometheus-k8s-db-prometheus-k8s-1         100G       RWO                           10m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html#persistent-storage&quot;&gt;https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html#persistent-storage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jia Jun Ng (JJ)</name></author><category term="redhat" /><category term="openshift" /><category term="prometheus" /><category term="openshift monitoring" /><summary type="html">This post covers the configuration of persistent storage with HostPath for OpenShift Monitoring stack. In OpenShift v3.11, Prometheus cluster monitoring is now fully supported and deployed by default. As the monitoring stack is fairly new, there are many on-going discussions on supported storage plugins. And the documentation does not cover the detailed steps of setting up the storage. The configurability is also limited with Operator in placed which “protects” the configuration. Through various implementations with HostPath requirement, I have figured a way to make HostPath storage works with Operator.</summary></entry></feed>